{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV\n",
    "\n",
    "\n",
    "[OpenCV](https://opencv.org/) is a free and open-source image processing and computer vision library.  OpenCV has over 2500 optimized algorithms written in C++, but it provides Python wrappers. Therefore, this library can be used in your Python\n",
    "programs. `opencv-python` is the Python package that contains pre-built OpenCV with dependencies and Python bindings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV-Python Installation\n",
    "\n",
    "First install the Anaconda/Miniconda distribution of Python: (https://www.anaconda.com/products/distribution) or (https://docs.conda.io/en/main/miniconda.html)\n",
    "\n",
    "\n",
    "It is recommended to install OpenCV in a separate virtual environment\n",
    "\n",
    "To create a new conda environment, issue the following command on Anaconda prompt:\n",
    "\n",
    "`conda create --name myenv `\n",
    "\n",
    "`conda activate myenv`\n",
    "\n",
    "More on managing conda environments [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)\n",
    "\n",
    "\n",
    "Then install OpenCV with the command `pip install opencv-python`\n",
    "\n",
    "\n",
    "Link to OpenCV-Python Tutorial [here](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OpenCV's Python module is called cv2 even though we are using\n",
    " OpenCV 4.x and not OpenCV 2.x. Historically, OpenCV had two Python\n",
    " modules: cv2 and cv. The latter wrapped a legacy version of OpenCV\n",
    " implemented in C. Nowadays, OpenCV has only the cv2 Python module,\n",
    " which wraps the current version of OpenCV implemented in C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an image using `cv2.imread()`: Many other python libs also have an imread fn: matplotlib, pillow, scikit-image.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/logo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dimensions of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size #total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the image using `cv2.imshow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Logo image\", img)\n",
    "\n",
    "# cv2.waitKey() is a keyboard binding function.\n",
    "# The argument for waitKey is a number of milliseconds to wait for keyboard input. By\n",
    "# default, it is 0, which is a special value meaning infinity. The return value is either -1\n",
    "# (meaning that no key has been pressed) or an ASCII keycode, such as 27 for Esc.\n",
    "#waitKey only captures input when an OpenCV window has focus.\n",
    "cv2.waitKey(10000)\n",
    "\n",
    "cv2.destroyWindow('Logo image') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now display the image using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?? <br>\n",
    "For historical reasons, OpenCV defaults to BGR format instead of usual RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV implements literally hundreds of formulas that pertain to the conversion of color models. We can convert the BGR image to RGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(img_gray); \n",
    "plt.colorbar();#default color map is 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_gray, cmap='gray');\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"gray image\", img_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a random image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rim=np.random.randint(0, 256, (200,300))\n",
    "plt.imshow(rim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save images using `cv2.imwrite()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('data/rand_im.jpg', rim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV supports a number of formats such as jpg, png, bmp, tiff,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('data/rand_im.bmp', rim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image operations as Numpy array operations\n",
    "Let us draw a black cross over the random image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rim[75:125, :] = 0\n",
    "rim[:, 100:200] = 0\n",
    "plt.imshow(rim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Draw letter H in a red color in blue background on a 5x4 RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[] \n",
    "h= np.zeros((5,4,3), dtype=np.uint8)\n",
    "plt.imshow(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if `cv2.imshow()` is used to display `h`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"h\", h) #image is too small\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom window\n",
    "cv2.namedWindow('custom window', cv2.WINDOW_NORMAL ) # WINDOW_NORMAL enables you to resize the window\n",
    "cv2.imshow('custom window', h)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you will have to play with certain regions of images. It can be done with Numpy slicing. Here, I am selecting a 50x50 region on the top-left of logo.png and pasting it to the bottom right corner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=cv2.imread('data/logo.png')\n",
    "im[-50:, -50:, :] = im[:50, :50, :]\n",
    "plt.imshow(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, we are using `cv2.imread()` and `plt.imshow()`. Hence B and R channels are reversed in the displayed image. How can you reverse the R and B channels using array operations on `im` so that `plt.imshow` shows the correct colors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try this\n",
    "b = im[:, :, 0]\n",
    "r = im[:, :, 2]\n",
    "\n",
    "im[:, :, 0] = r\n",
    "im[:, :, 2] = b\n",
    "plt.imshow(im);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't work as expected!! What went wrong?\n",
    "In Numpy, slice of an array is a view into the same data. Not copies. Unlike Matlab.\n",
    "`b` is a view into `im[:, :, 0]`. When `im[:,:,0]` is modified, `b` is also modified.\n",
    "If we want a copy, we have to use the `copy` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = im[:, :, 0].copy()\n",
    "r = im[:, :, 2]\n",
    "im[:, :, 0] = r\n",
    "im[:, :, 2] = b\n",
    "plt.imshow(im);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic with Images\n",
    "OpenCV does *saturation arithmetic* when performing arithmetic operation on images as opposed to *modular arithmetic* done by Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[250, 100], [250, 100]], dtype=np.uint8)\n",
    "y = np.array([[10, 10],[10,10]], dtype=np.uint8)\n",
    "x + y #Numpy addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.add(x, y) #OpenCV addition-which is what we normally need with images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/lena.jpg')\n",
    "\n",
    "# Convert BGR image to RGB:\n",
    "img_RGB = img[:, :, ::-1]\n",
    "plt.imshow(img_RGB);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 60 to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.full(img.shape, 60, dtype=np.uint8)\n",
    "img_add = cv2.add(img, M)\n",
    "plt.imshow(img_add[:, :, ::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Subtract 100 from all channels in all pixels in `img` using `cv2.subtract()` and display using `plt.imshow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.full(img.shape, 100, dtype=np.uint8)\n",
    "img_sub = cv2.subtract(img,M)\n",
    "plt.imshow(img_sub[:, :, ::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Blending\n",
    "Image blending is also image addition, but different weights are given to the images.\n",
    "\n",
    "This function is commonly used to get the\n",
    "output from the Sobel operator.The Sobel operator is used for edge detection, where it creates an image emphasizing\n",
    "edges. The Sobel operator uses two 3 × 3 kernels, which are convolved with the original\n",
    "image in order to calculate approximations of the derivatives, capturing both horizontal\n",
    "and vertical changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/pic1.jpg')\n",
    "img2 = cv2.imread('data/pic2.jpg')\n",
    "\n",
    "#alpha = 0.3, 0.7=1-0.3; make sure those values add to 1 if you want conserve brightness\n",
    "blended = cv2.addWeighted(img1, 0.3, img2, 0.7, 0)\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img1[:, :, ::-1])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(img2[:, :, ::-1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(blended[:, :, ::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Filtering\n",
    "The `cv2.GaussianBlur()`  blurs an image by using a Gaussian kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baboon = plt.imread('data/baboon.jpg')\n",
    "\n",
    "#GaussianBlur(\tsrc, ksize, sigmaX, sigmaY,...\t)\n",
    "#when sigmaX=0, it is computed from kernel size\n",
    "babblur = cv2.GaussianBlur(baboon,(29,29),0)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(baboon)\n",
    "plt.subplot(122)\n",
    "plt.imshow(babblur);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cv2.filter2D()` function can be used to apply an arbitrary kernel to an\n",
    "image, convolving the image with the provided kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom kernel; simple box-car in this case\n",
    "kernel = np.ones((15,15))\n",
    "kernel /= kernel.size #normalize kernel so as not to scale image intensity\n",
    "\n",
    "babblur2 = cv2.filter2D(baboon,-1,kernel) #the argument -1 is for ddepth=-1; the output image will have the same depth as the source-uint8\n",
    "# each channel is processed independently\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(baboon)\n",
    "plt.subplot(122)\n",
    "plt.imshow(babblur2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other smoothing filters such as median blur and bilateral filter are also available. See the [tutorial](https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing camera frames\n",
    "The `cv2.VideoCapture()` object allows you to capture videos from different sources, such as cameras, video files and image sequences. When capturing frames from a camera connected to your computer, you have to give the camera index as the argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0) #calling the constructor, 0 is the camera index\n",
    "# Get some properties of VideoCapture using get() method\n",
    "frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Print these values:\n",
    "print(f\"CV_CAP_PROP_FRAME_WIDTH: {frame_width}\")\n",
    "print(f\"CV_CAP_PROP_FRAME_HEIGHT : {frame_height}\")\n",
    "print(f\"CAP_PROP_FPS : {fps}\")\n",
    "\n",
    "ret, frame = capture.read()\n",
    "while ret:\n",
    "    cv2.imshow('Input frame from the camera', frame)\n",
    "    # Capture frame-by-frame from the camera\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    " \n",
    " \n",
    "# Release everything:\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "Deep Learning has changed Computer Vision forever!\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/) is an open source Deep Learning platform developed by Google. \n",
    "\n",
    "TensorFlow Hub('https://tfhub.dev') is a repository of pretrained models curated by Google.\n",
    "\n",
    "### Image Classification using pre-trained model from Tensorflow Hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle='https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = hub.load(model_handle)\n",
    "tf.saved_model.save(classifier, './model') #we can later use classifier=tf.saved_model.load('./model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us input an image to the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./data/tiger.jpg')[...,::-1]\n",
    "image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into shape [batch_size, height, width, num_channels]\n",
    "image = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though tf hub doc page mentions that input shape should be 384x384, it is seen that the model accepts any shape\n",
    "# Also, the number of outputs is seen to be 1000, instead of 1001 as mentioned in the doc\n",
    "# some models need input to be resized. (eg: mobilenet) We can use tf.resize() for resizing\n",
    "#To resize without changing aspect ratio, use tf.resize_with_pad() or tf.resize_with_crop_or_pad()\n",
    "output=classifier(image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=tf.nn.softmax(output)\n",
    "probs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.numpy().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels.txt file contains the 1000 class labels of ImageNet. Some models include an additional 'background' class in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/labels.txt') as f: #You use a with statement to create a context manager to ensure the file is closed as soon as it’s no longer needed.\n",
    "  labels = f.readlines()\n",
    "  classes = [l.strip() for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = tf.argsort(probs, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\n",
    "\n",
    "for i in top_5:\n",
    "    print(classes[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn how to apply transfer learning to models in TFHub, see [this](https://www.tensorflow.org/hub/tutorials/tf2_image_retraining) tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object detection using pre-trained model from Tensorflow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = hub.load(\"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\").signatures['default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./data/Naxos_Taverna.jpg')[...,::-1]\n",
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "converted_img.dtype, converted_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detector(converted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {key:value.numpy() for key,value in result.items()}\n",
    "\n",
    "print(f\"Found {len(result['detection_scores'])} objects.\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_h, im_w, _ = img.shape\n",
    "\n",
    "boxes = (np.array([im_h, im_w, im_h, im_w])*result[\"detection_boxes\"]).astype('int')\n",
    "scores = (100*result[\"detection_scores\"]).round(1)\n",
    "class_names = result[\"detection_class_entities\"].astype('str')\n",
    "\n",
    "for score, box, class_name in zip(scores, boxes, class_names):\n",
    "    print(score, box, class_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loop throughout the detections and place a box around it\n",
    "img_boxes=img.copy()\n",
    "for score, (ymin, xmin, ymax, xmax), label in zip(scores, boxes, class_names):\n",
    "\n",
    " \n",
    "    if score > 20:\n",
    "        cv2.rectangle(img_boxes, (xmin, ymin), (xmax, ymax), (0, 255, 0), 4)      \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        txt=label+':'+ str(score)\n",
    "        cv2.putText(img_boxes, txt, (xmin, ymax-10),\n",
    "                        font, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(img_boxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting objects in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "im_w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "im_h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "while True:\n",
    "    #Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    " \n",
    "    #Convert img to RGB\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "  \n",
    "    # convert to float and add dimension\n",
    "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "    \n",
    "    result = detector(converted_img)\n",
    "\n",
    "    result = {key:value.numpy() for key,value in result.items()}\n",
    "\n",
    "    boxes = (np.array([im_h, im_w, im_h, im_w])*result[\"detection_boxes\"]).astype('int')\n",
    "    class_names = result[\"detection_class_entities\"].astype('str')\n",
    "    scores = (100*result[\"detection_scores\"]).round(1)\n",
    "\n",
    "    img_boxes=frame.copy()\n",
    "    for score, (ymin, xmin, ymax, xmax), label in zip(scores, boxes, class_names):\n",
    " \n",
    "        if score > 20:\n",
    "            cv2.rectangle(img_boxes, (xmin, ymin), (xmax, ymax), (0, 255, 0), 4)      \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            txt=label+':'+ str(score)\n",
    "            cv2.putText(img_boxes, txt, (xmin, ymax-10),\n",
    "                            font, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #Display the resulting frame\n",
    "    cv2.imshow('detections', img_boxes)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving camera frames:\n",
    "`cv2.VideoWriter` object can be used to write frames to a video file. The video's file name and codec must be specified as arguments to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "# Get some properties of VideoCapture (frame width, frame height and frames per second (fps)):\n",
    "frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "writer = cv2.VideoWriter('MyOutputVid.mp4', \n",
    "cv2.VideoWriter_fourcc(*'MP4V'), # FourCC is a 4-byte code used to specify the video codec-file ext and codec should match\n",
    "int(fps), (int(frame_width), int(frame_height)))\n",
    "\n",
    "# videoWriter = cv2.VideoWriter(\n",
    "# 'MyOutputVid1.avi', cv2.VideoWriter_fourcc('I','4','2','0'),\n",
    "# int(fps), (int(frame_width), int(frame_height)))\n",
    "\n",
    "frame_number=0\n",
    "while frame_number < 150:\n",
    "    # Capture frame-by-frame from the camera\n",
    "    ret, frame = capture.read()    \n",
    "    writer.write(frame)\n",
    "\n",
    "    frame_number +=1\n",
    " \n",
    "# Release everything:\n",
    "capture.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a video file\n",
    "`cv2.VideoCapture` also allows us to read a video file. To read a video file, the\n",
    "path to the video file should be passed instead of the camera's device index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('MyOutputVid.mp4')\n",
    "\n",
    "ret, frame = capture.read()\n",
    "\n",
    "while ret:\n",
    "    cv2.imshow('Frame from video file', frame)\n",
    "    ret, frame = capture.read()    \n",
    "       \n",
    "    if cv2.waitKey(33) == ord('q'): #30 frames per 1000 ms ~= 33 ms per frame\n",
    "        break\n",
    " \n",
    "# Release everything:\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny Edge Detection\n",
    "(https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('data/lena.jpg')\n",
    "canny_edge= cv2.Canny(img, 100, 200) # Canny in one line!\n",
    "plt.imshow(canny_edge, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Scripts\n",
    "Many sample programs are included in the OpenCV's source code archive. To dowload the source code, go to (https://opencv.org/releases/) and download **Sources**. It is a zip file (90 MB). Unzip it and find the samples scripts in `opencv/samples/python` folder.\n",
    "Try running a sample program, for example, `hist.py`.\n",
    "\n",
    "Note that many of the sample scripts require command line arguments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c76692b2dde9e57b9b6d47861cbdd207e0e7a5a89ac4a3318b52bead500363a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
